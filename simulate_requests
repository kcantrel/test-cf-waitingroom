#!/bin/bash
#
# This script is used to put load on a Waiting Room endpoint and then record
# how long each request is kept in the waiting room.
#
# It outputs to standard output:
#   ? - Did not receive an HTTP status from the curl command.
#   o - Did not receive a 200 HTTP status from the origin.
#   . - A request went straight to the origin.
#   a-zA-Z1-0 - How long a request stayed in the waiting room. a - 10 seconds, b = 20 seconds, etc.
#
# It also creates a file called "results" that contains a line for each request that contains:
# <JobID>,<Status>,<wait_time>,<wr_cnt_before>,<wr_cnt_after>,<start_time>,<end_time>
# Where:
#     <JobID> is a fixed string followed by a sequence number
#     <Status> is 0 if everything went okay. 1 if there was HTTP status code, and 2 if the status code wasn't 200.
#     <wait_time> is the number of seconds the request was in the waiting room.
#     <wr_cnt_before - Is the number of requests that are currently looping waiting to make it to the origin when this request started.
#     <wr_cnt_after - Is the number of requests that are currently looping waiting to make it to the origin when this request made it to the origin.
#     <start_time> - Is the time, in UNIX epoch format, when the request started.
#     <end_time> - Is the time, in UNIX epoch format, when the request finished.
#
################################################################################
#
################################################################################
# The script get the current number of jobs in the queue.
# Uses mkdir as a locking mechamism.
################################################################################
get_wr_cnt () {
  lockfile=$lfc
  i=0
  while [ $i -lt 100 -a $i -ge 0 ]; do
    if mkdir $lockfile > /dev/null 2>&1; then
      if [ -s $wrc ]; then
        cat $wrc
      else
        echo "0"
      fi
      i=-1
    else
      let i+=1
      sleep .1
    fi
  done

  if [ $i -ge 100 ]; then
    echo "Error, could not get lock on file '$lockfile'." 1>&2
    exit 1
  fi

  rmdir $lockfile
}

################################################################################
# The script subtracts 1 from the current number of jobs in the queue.
# Uses mkdir as a locking mechamism.
################################################################################
sub_wr_cnt () {
  lockfile=$lfc
  i=0
  while [ $i -lt 100 -a $i -ge 0 ]; do
    if mkdir $lockfile > /dev/null 2>&1; then
      if [ -s $wrc ]; then
        x=$(cat $wrc)
      else
        x=0
      fi
      let x-=1
      echo $x > $wrc
      i=-1
    else
      let i+=1
      sleep .1
    fi
  done

  if [ $i -ge 100 ]; then
    echo "Error, could not get lock on file '$lockfile'." 1>&2
    exit 1
  fi

  rmdir $lockfile
}

################################################################################
# The script adds 1 to the current number of jobs in the queue.
# Uses mkdir as a locking mechamism.
################################################################################
add_wr_cnt () {
  lockfile=$lfc
  i=0
  while [ $i -lt 100 -a $i -ge 0 ]; do
    if mkdir $lockfile > /dev/null 2>&1; then
      if [ -s $wrc ]; then
        x=$(cat $wrc)
      else
        x=0
      fi
      let x+=1
      echo $x > $wrc
      i=-1
    else
      let i+=1
      sleep .1
    fi
  done

  if [ $i -ge 100 ]; then
    echo "Error, could not get lock on file '$lockfile'." 1>&2
    exit 1
  fi

  rmdir $lockfile
}

################################################################################
# Update results file
# Uses mkdir as a locking mechamism.
################################################################################
update_results () {
  lockfile=$lfr
  i=0
  while [ $i -lt 100 -a $i -ge 0 ]; do
    if mkdir $lockfile > /dev/null 2>&1; then
      echo "$*" >> $results
      i=-1
    else
      let i+=1
      sleep .1
    fi
  done

  if [ $i -ge 100 ]; then
    echo "Error, could not get lock on file '$lockfile'." 1>&2
    exit 1
  fi

  rmdir $lockfile
}

################################################################################
# This function is used to send a request to the end point. If it get a message
# that it is in the waiting room, it will keep trying until it gets through.
# It will output:
#   '?' - If the curl fails to get a value HTTP status code.
#   'o' - If it gets a HTTP status code that doesn't equal 200.
#   '.' - If it gets through the first time.
#   '[a-Z]' - Repersenting the number of times it tried to get through, in 
#             10 second intervals. So for example 'c' means it got through
#             on the 3 attempt (30 seconds).
################################################################################
send_request() {

  job=$1
  output=output-$job.$BASHPID
  cookie_jar=cookie_jar-$job.$BASHPID
  letters=(. a b c d e f g h i j k l m n o p q r s t u v w x y z A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9) 
  wr_dur=0

  in_waiting_room=true # gaurantee once through the loop.
  st=$(date '+%s')
  bwrc=$(get_wr_cnt)
  while [ $in_waiting_room = "true" ]; do
    in_waiting_room=false
    curl -sv -b $cookie_jar -c $cookie_jar $url > $output 2>&1
    hs=$(egrep '^< HTTP/' $output | awk '{print $3}')
    if [ -z "$hs" ]; then
      echo -n "?"
      awrc=$(get_wr_cnt)
      update_results "$job,1,0,$bwrc,$awrc,$st,$st"
      rm -f $cookie_jar # $output  # Keep the output file for diagnoses purposes.
      return
    elif [ "$hs" -ne 200 ]; then
      echo -n "o"
      awrc=$(get_wr_cnt)
      update_results "$job,2,0,$bwrc,$awrc,$st,$st"
      rm -f $cookie_jar # $output  # Keep the output file for diagnoses purposes.
      return
    elif fgrep 'Your estimated wait time is' $output > /dev/null; then
      in_waiting_room=true
      if [ $wr_dur -eq 0 ]; then
        add_wr_cnt
      fi
    else # Assuming out of the waiting room
      let twt=wr_dur*10
      if [ $wr_dur -gt ${#letters[@]} ]; then
        echo -n '!'
      else
        echo -n "${letters[$wr_dur]}"
      fi
      if [ $wr_dur -gt 0 ]; then
        sub_wr_cnt
      fi
    fi
    if [ $in_waiting_room = "true" ]; then
      let wr_dur+=1
      sleep 10
    fi
  done

  awrc=$(get_wr_cnt)
  et=$(date '+%s')
  let wt=et-st
  update_results "$1,0,$wt,$bwrc,$awrc,$st,$et"

  rm -f $output $cookie_jar
}

################################################################################
# This function send requests to the endpoint.
# Expected Argurments:
#   1 = The number of requests
#   2 = The time to wait between requests
#   3 = A prefix to make the statistics file unique.
################################################################################
send_requests () {
  nr=$1
  wait_time=$2
  pre=$3

  for i in $(seq -f "%05g" 1 $nr); do
    send_request $pre-$i &
    sleep $wait_time
  done
}

################################################################################
# This function just output the usage statement.
################################################################################
usage() {
    cat <<EOF
Usage: $(basename $0) -n <num_secs_p1> -m <num_secs_p2> -s <sleep_time_p1> -o <results> URL
  where:
    num_secs_p1 - is  the number of seconds to send requests during phase 1.
    num_secs_p2 - is  the number of seconds to send requests during phase 2, which is fixed at 1 RPS.
    sleep_time_p1 - is the amount of time to sleep between requests. Fractional time accepted (e.g. .3).
    results - is the file to store the per request statistics.
    URL - is the URL of the end point you want to send requests to.

Here is how sleep_time equates to RPS:
        .05 = 20   rps or 1200 rpm
        .1  = 10   rps or 600 rpm
        .2  =  5   rps or 300 rpm
        .3  =  3.3 rps or 200 rpm
        1   =  1   rps or 60 rpm
EOF
}

################################################################################
# Main logic starts here.
################################################################################
#
# Pointers to temporary files.
wrc=.waiting_room_cnt.$$
tmpout=.tmpout.$$
lfc=.lockfile-cnt
lfr=.lockfile-results
trap 'rm -f $wrc $tmpout; rmdir $lfc $lfr > /dev/null 2>&1' 0
#
# Ensure there aren't any left over lockfiles from a previously aborted run.
rmdir $lfc $lfr > /dev/null 2>&1
# Since the old version of the script used files instead of directories, ensure
# they don't exist either.
rm -f $wrc $tmpout $lfc $lfr > /dev/null 2>&1
# 
#
if which curl > /dev/null  2>&1; then
  :
else
  echo "Error, the "curl" command has to be in the command serach path."
  exit 1
fi
#
# Processs command line arguments.
while getopts "n:m:s:o:h" option; do
  case "${option}" in
    h) usage
      exit
      ;;
    n) nsph1=${OPTARG}
       ;;
    m) nsph2=${OPTARG}
       ;;
    s) wait_time=${OPTARG}
       ;;
    o) results=${OPTARG}
       ;;
    *) 
       echo "unknown option: '${option}'"
       usage 2>&1
       exit 1
       ;;
  esac
done

shift $((OPTIND-1))
url="$1"
#
# Check Arguments
if [ -z "$url" -o -z "$nsph1" -o -z "$nsph2" -o -z "$wait_time" -o -z "$results" ]; then 
  echo "Error, missing parameter." 2>&1
  usage 2>&1
  exit
fi
#
# Truncate the results file with the column headers.
if echo "job,status,wait_time,wr_cnt_before,wr_cnt_after,start_time,end_time" > $results 2> $tmpout; then
  :
else
  echo "Error, failed to write to results file: '$results'."
  cat $tmpout
  exit 1
fi

rps=$(echo "scale=2; 1/$wait_time" | bc -l)
rpm=$(echo "scale=0; $rps*60" | bc -l | sed -e 's/\.[0-9]*//')
nr=$(echo "$rps*$nsph1" | bc -l | sed -e 's/\.[0-9]*//')
echo "Sending $nr requests to $url at a rate of $rps per second. Or $rpm per minute."
echo ""

date
send_requests $nr $wait_time "main"
echo ""
date

echo ""
echo "Now doing 1 request per second for $nsph2 seconds."
send_requests $nsph2 1 "post"
echo ""
date
echo ""
echo "Waiting for jobs to finish"
wait
echo ""
date
